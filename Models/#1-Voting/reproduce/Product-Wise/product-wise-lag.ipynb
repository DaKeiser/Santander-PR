{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport csv\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Renaming columns for ease of use\ndef rename_columns(data):\n    data.rename(columns = {\"fecha_dato\":\"time_series\",\"ncodpers\":\"customer_code\",\"ind_empleado\":\"employee_index\",\\\n                       \"pais_residencia\":\"country_residence\",\"sexo\":\"gender\",\"fecha_alta\":\"Date_First_Customer\",\\\n                       \"ind_nuevo\":\"New_Customer_ind\",\"antiguedad\":\"Seniority\",\"indrel\":\"primary_cust\",\\\n                       \"ult_fec_cli_1t\":\"last_date_primary\",\"indrel_1mes\":\"customer_type\",\"tiprel_1mes\":\"cust_rel_type\",\\\n                       \"indresi\":\"residence_index\",\"indext\":\"foriegn_index\",\"conyuemp\":\"spouse_index\",\"canal_entrada\":\"channel_by_cust_joined\",\\\n                       \"indfall\":\"deceased_index\",\"tipodom\":\"primary_address\",\"cod_prov\":\"province_code\",\"nomprov\":\"province_name\",\\\n                       \"ind_actividad_cliente\":\"activity_index\",\"renta\":\"gross_income\",\"segmento\":\"segmentation\",\\\n                       \"ind_ahor_fin_ult1\":\"savings_account\",\"ind_aval_fin_ult1\":\"guarantees\",\"ind_cco_fin_ult1\":\"current_account\",\\\n                       \"ind_cder_fin_ult1\":\"derivative_account\",\"ind_cno_fin_ult1\":\"payroll_account\",\"ind_ctju_fin_ult1\":\"jnr_account\",\\\n                       \"ind_ctma_fin_ult1\":\"mas_particular_account\",\"ind_ctop_fin_ult1\":\"particular_account\",\"ind_ctpp_fin_ult1\":\"particular_Plus_Account\",\\\n                       \"ind_deco_fin_ult1\":\"short_term_deposits\",\"ind_deme_fin_ult1\":\"medium_term_deposits\",\\\n                       \"ind_dela_fin_ult1\":\"long_term_deposits\",\"ind_ecue_fin_ult1\":\"e_account\",\"ind_fond_fin_ult1\":\"funds\",\\\n                       \"ind_hip_fin_ult1\":\"mortgage\",\"ind_plan_fin_ult1\":\"pensions\",\"ind_pres_fin_ult1\":\"loans\",\\\n                       \"ind_reca_fin_ult1\":\"taxes\",\"ind_tjcr_fin_ult1\":\"credit_card\",\"ind_valo_fin_ult1\":\"securities\",\\\n                       \"ind_viv_fin_ult1\":\"home_account\",\"ind_nomina_ult1\":\"payroll\",\"ind_nom_pens_ult1\":\"pensions1\",\n                       \"ind_recibo_ult1\":\"direct_debit\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_train = pd.read_csv(\"../input/santander-pr/train.csv\")\nrename_columns(main_train)\nmain_train.dtypes\n#Removing bad rows which have all attributes empty after verifying such row d.n.e. in test data\nmain_train = main_train[main_train['employee_index'].notna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"products = [\"savings_account\",\"guarantees\",\"current_account\",\"derivative_account\",\\\n           \"payroll_account\",\"jnr_account\",\"mas_particular_account\",\"particular_account\",\\\n           \"particular_Plus_Account\",\"short_term_deposits\",\"medium_term_deposits\",\"long_term_deposits\",\\\n           \"e_account\",\"funds\",\"mortgage\",\"pensions\",\\\n            \"loans\",\"taxes\",\"credit_card\",\"securities\",\\\n            \"home_account\",\"payroll\",\"pensions1\",\"direct_debit\"]\nnon_pro = [x for x in main_train.columns if x not in products+['spouse_index','province_name','last_date_primary','province_name','customer_type','cust_rel_type','Date_First_Customer']]\nnon_pro = non_pro + ['Date_first_customer_year','time_series_month']\nprint(len(products),len(non_pro))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imputing based on what we presented in our initial presentations\ndef preprocessing_dat(data_given):\n    data = data_given\n    data['age'] = pd.to_numeric(data['age'])\n    data['age_square'] = np.square(data['age'])\n    data.loc[(data.age < 40) & (data.segmentation.isnull()),'segmentation'] = '03 - UNIVERSITARIO'\n    data.loc[(data.age < 50) & (data.segmentation.isnull()) & (data.age >=40),'segmentation'] = '02 - PARTICULARES'\n    data.loc[(data.segmentation.isnull()) & (data.age >=50),'segmentation'] = '01 - TOP'\n    data.loc[data['province_code'].isnull(), 'province_code'] = 28.0\n    data.gross_income = data.groupby('province_code')['gross_income'].apply(lambda x : x.fillna(x.median()))\n    data['gross_income_log'] = np.log(data.gross_income)\n    data.gross_income = data.gross_income.fillna(data.gross_income.median())\n    data.loc[(data['gender'].isna()) & (data['customer_code']%2 == 0),'gender'] = 'H'\n    data.loc[(data['gender'].isna()) & (data['customer_code']%2 == 1),'gender'] = 'V'\n    data['Date_First_Customer'] = pd.to_datetime(data['Date_First_Customer'])\n    data.channel_by_cust_joined = data.groupby(data.Date_First_Customer.dt.year)['channel_by_cust_joined'].apply(lambda x : x.fillna(x.mode()[0]))\n    data.channel_by_cust_joined = data.channel_by_cust_joined.fillna(data.channel_by_cust_joined.mode()[0])\n    data['Seniority'] = data['Seniority'].astype('int32')\n    data['time_series'] = pd.to_datetime(data['time_series'])\n    data['Date_First_Customer'] = pd.to_datetime(data['Date_First_Customer'])\n    data['time_series_month'] = data['time_series'].apply(lambda x : x.month)\n    data['time_series_month_sq'] = np.square(data['time_series'].apply(lambda x : x.month))\n    data['Date_first_customer_year'] = data['Date_First_Customer'].apply(lambda x : x.year)\n    data['Date_first_customer_month'] = data['Date_First_Customer'].apply(lambda x : x.month)\n    data['cust_rel_type'] = data['cust_rel_type'].astype('str')\n    data['cust_rel_type'].fillna(data['cust_rel_type'].mode()[0])\n    data.drop(columns=['spouse_index','province_name','last_date_primary','customer_type','Date_First_Customer'],inplace=True)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Few product columns are null we will them with 0's because their majority in every product\ndef preprocess_products(data_given):\n    data = data_given\n    data.payroll = data.payroll.fillna(0)\n    data.pensions = data.pensions.fillna(0)\n    data.pensions1 = data.pensions1.fillna(0)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Customised lag function where we can specify how months lag and what attributes lag do we need\ndef lagn(df,n=4,cols=products):\n    time_groups = df.groupby('time_series')\n    dfs = []\n    for time in time_groups.groups:\n        mini_df = time_groups.get_group(time)\n        for off in range(1,n+1):\n            prev_month = time - pd.DateOffset(months=off)\n            lag_products = main_train.loc[main_train['time_series']==prev_month,cols+['customer_code']]\n            if(off == 1):\n                lag_names = [x + '_lag' for x in cols]\n            else:\n                lag_names = [x + '_lag'+str(off) for x in cols]\n            rename_col = {cols[i]: lag_names[i] for i in range(len(cols))}\n            lag_products.rename(columns=rename_col,inplace=True)\n            mini_df = pd.merge(mini_df,lag_products,on='customer_code',how='left')\n        mini_df.fillna(0,inplace=True)\n        dfs.append(mini_df)\n    resultant_df = pd.concat(dfs)\n    resultant_df.drop(columns=['time_series','customer_code'],inplace=True)\n    return resultant_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Customised exponential smoothing addition for the passed df\nalpha_columns = ['alpha_0.03','alpha_0.1','alpha_0.3','alpha_0.9']\nalpha_vals = np.array([0.03,0.1,0.3,0.9])\n\ndef alpha_lag(df,cols=alpha_columns,cur_product='savings_account'):\n    time_groups = df.groupby('time_series')\n    dfs = []\n    for time in time_groups.groups:\n        mini_df = time_groups.get_group(time)\n        prev_month = time - pd.DateOffset(months=1)\n        lag_products = main_train.loc[main_train['time_series']==prev_month,cols+['customer_code']]\n        lag_names = [x + '_lag' for x in cols]\n        rename_col = {cols[i]: lag_names[i] for i in range(len(cols))}\n        lag_products.rename(columns=rename_col,inplace=True)\n        mini_df = pd.merge(mini_df,lag_products,on='customer_code',how='left')\n        mini_df.fillna(0,inplace=True)\n        for i in range(len(cols)):\n            mini_df[cols[i]] = alpha_vals[i]*mini_df[cur_product+'_lag'] + (1-alpha_vals[i])*mini_df[cols[i]+'_lag']\n        dfs.append(mini_df)\n    resultant_df = pd.concat(dfs)\n    resultant_df.drop(columns=['time_series','customer_code'],inplace=True)\n    return resultant_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_train = preprocessing_dat(main_train)\nmain_train = preprocess_products(main_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/santander-pr/test.csv\")\nrename_columns(test)\n#Final df that we will be writing to csv, storing the id's in the order of test to avoid confusion\ncompute_df = pd.DataFrame()\ncompute_df['ncodpers'] = test['customer_code']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = preprocessing_dat(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Block for label encoding\nobj_col = list(main_train.select_dtypes(['object']).columns) \nprint(obj_col)\nle = preprocessing.LabelEncoder()\nmain_train[obj_col] = main_train[obj_col].apply(le.fit_transform)\ntest[obj_col] = test[obj_col].apply(le.fit_transform) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Min max scalar attribute wise as doing as a whole blows up memory\nimport sys\nnon_prod_cols = [item for item in main_train.columns if item not in products+['customer_code','time_series']]\n\n\nfor c in non_prod_cols:\n    min_max_scaler = preprocessing.MinMaxScaler()\n    \n    x = main_train[c].values\n    x = x.reshape(-1, 1)\n    main_train[c] = min_max_scaler.fit_transform(x)\n\n    x = test[c].values\n    x = x.reshape(-1, 1)\n    test[c] = min_max_scaler.transform(x)\n    \n    sys.stdout.write(\"\\r\" + \"Processed \"+ str(c))\n    sys.stdout.flush()\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Block to review if we can remove any unused variables in RAM\nfrom __future__ import print_function \nimport sys\n\nlocal_vars = list(locals().items())\nfor var, obj in local_vars:\n    print(var, sys.getsizeof(obj))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times = ['2016-03-28','2016-01-28','2016-03-28','2016-01-28','2015-09-28','2016-04-28','2015-08-28','2015-10-28',\\\n         '2015-12-28','2016-04-28','2016-04-28','2016-04-28','2015-08-28','2016-03-28','2015-08-28','2015-05-28',\\\n         '2015-05-28','2016-03-28','2016-03-28','2015-05-28','2016-03-28','2015-12-28','2016-03-28','2016-04-28']\nsample_time = dict(zip(products,times))\nsample_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = lagn(test)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Unused set of models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler'''\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n#Dictionary for storing probabilities of class predictions of validation set\nprobabilities = dict()\n#Dictionary for storing probabilities of predictions of validation set\npredictions = dict()\n#Dictionary for storing models trained specific to each product\nmodels = dict()\ntscv = TimeSeriesSplit(n_splits=5)\n    \nfor category in products:\n    print('**Processing {} product ...**'.format(category))\n    #sampling data as per the product on fly as per the months arrived from EDA\n    target_time = pd.to_datetime(sample_time[category])\n    train_data = main_train.loc[(main_train.time_series == target_time)]\n    #Adding 4 month lag columns from the month of the sample train data\n    train_data = lagn(train_data)\n    non_prod_cols = [item for item in train_data.columns if item not in products+['customer_code','time_series']]\n    y = train_data[category]\n    X = train_data[non_prod_cols]\n    for tr_index, val_index in tscv.split(X):\n        X_tr, X_val = X.iloc[tr_index], y.iloc[tr_index]\n        y_tr, y_val = X.iloc[val_index], y.iloc[val_index]\n    clf = XGBClassifier(max_depth=10, learning_rate = 0.05, subsample = 0.8, colsample_bytree = 0.8, n_estimators=100,verbosity=1)\n    clf.fit(X_tr, X_val)\n    prediction = clf.predict(y_tr)\n    predict_probability = clf.predict_proba(y_tr)\n    predictions[category] = prediction\n    probabilities[category] = predict_probability\n    models[category] = clf\n    print('Probability {} accuracy is {}'.format(category,predict_probability))\n    print(\"\\n\")\n    print('Test accuracy is {}'.format(accuracy_score(y_val, prediction)))\n    print(\"\\n\")\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing unsued variables to make better use of memory\ndel probabilities\ndel train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''x = test.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ntest = pd.DataFrame(x_scaled,columns=test.columns,index=test.index)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prediction = {}\ntest_probas = {}\nfor category in products:\n    prediction = models[category].predict(test)\n    predict_probability = models[category].predict_proba(test)\n    test_prediction[category] = prediction\n    test_probas[category] = predict_probability[:,1]\n    print('Probability {} is {}'.format(category,predict_probability))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_probas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making a df from the probabilities\nresults_df = pd.DataFrame(test_probas)\nresults_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame(abs(results_df.values - test[products_lag].values))\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This function will be called inside pd.apply() to parallely compute top 5 suggests\ndef top5(row):\n    product_name = ['ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1','ind_cder_fin_ult1',\\\n               'ind_cno_fin_ult1','ind_ctju_fin_ult1','ind_ctma_fin_ult1','ind_ctop_fin_ult1',\\\n               'ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1','ind_dela_fin_ult1',\\\n               'ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1',\\\n               'ind_pres_fin_ult1','ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1',\\\n               'ind_viv_fin_ult1','ind_nomina_ult1','ind_nom_pens_ult1','ind_recibo_ult1']\n    width = len(product_name)\n    sort_index = np.argsort(row)\n    sort_index = sort_index[::-1]\n    product_list = [product_name[k] for k in sort_index[:5]]\n    recom_string = ' '.join(product_list)\n    return recom_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_df['changed'] = result.apply(top5,axis=1)\ncompute_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_df.to_csv('sample_lag.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Pipeline:\n\nStatic Data - Intutions - Predictions\n\nTo add:\nDynamic data(Time series) - Intutions - Add to prediction\n\nPredictions:\nIf user exists:\n    Get last month and compare change of product - done!\nElse:\n    Compare change of product with an array of 22(size of target variable) zeroes - done!\n    \n## Errors:\n\n1. Our predictions for long term deposists and pensions gave more than 2 classes.\n2. Encoding is very basic as of now (Evrything label encoded) same with normalization.\n3. Skews and asymetrix distributions\n4. New models\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['age'] = train['age'].astype(str).astype(int)\n# train['age'].dtype\n# fig, axes = plt.subplots(figsize=(5,8))\n# sns.boxplot(y='age',x='segmentation',data=train, ax=axes, showmeans=True)\n# empty_seg = train.loc[train['segmentation'].isnull(),['age']]\n# # print(train['segmentation'].unique())\n# train.loc[(train.age < 40) & (train.segmentation.isnull()),'segmentation'] = '03 - UNIVERSITARIO'\n# train.loc[(train.age < 50) & (train.segmentation.isnull()) & (train.age >=40),'segmentation'] = '02 - PARTICULARES'\n# train.loc[(train.segmentation.isnull()) & (train.age >=50),'segmentation'] = '01 - TOP'\n# print(train['segmentation'].unique())\n# s1 = train.loc[(train['province_code'].isnull()),'customer_code']\n# s2 = train.loc[(train['province_name'].isnull()),'customer_code']\n# print((s1 != s2).sum())\n# train['country_residence'].unique()\n# s1 = train['province_code'].value_counts()\n# s2 = train['province_name'].value_counts()\n# s2.index = s1.index\n# s1.equals(s2)\n# train.drop(columns='province_name',inplace=True) #This cannot be undone unless you run from beginning\n# s1\n# res_not_null = train.loc[train['province_code'].notnull(),['country_residence','foriegn_index','residence_index']]\n# print(res_not_null['country_residence'].unique())\n# res_null = train.loc[train['province_code'].isnull(),['country_residence','foriegn_index','residence_index']]\n# print(res_null['country_residence'].unique())\n# res_not_null.describe()\n# train['province_code'].value_counts().idxmax()\n# rem = train.loc[(train['province_code'].notnull()) & (train['country_residence'] != \"ES\"),['province_code','country_residence','foriegn_index','residence_index']]\n# train.loc[train['province_code'].isnull(), 'province_code'] = 28.0\n# train['province_code'].isnull().sum()\n# train['gross_income']\n# # sns.scatterplot(data=train,x='province_code',y='gross_income',hue='province_code',ax=axes)\n# province_income = train.loc[train['gross_income'].notnull(),:]\\\n#     .groupby('province_code').agg({'gross_income':[np.mean,np.median]})\n# province_income.sort_values(by=('gross_income',   'mean'),inplace=True)\n# province_income.reset_index(inplace=True)\n# # train['segmentation'].describe()\n# # print(province_income[('gross_income',   'mean')])\n# print(province_income['province_code'].shape,province_income[('gross_income',   'mean')].shape)\n# fig, axes = plt.subplots(figsize=(21,7))\n# province_income[('gross_income',   'mean')].plot(kind='bar',ax=axes)\n# fig, axes = plt.subplots(figsize=(21,7))\n# province_income.sort_values(by=('gross_income',   'median'),inplace=True)\n# province_income.reset_index(inplace=True)\n# province_income[('gross_income',   'median')].plot(kind='bar',ax=axes)\n# train.gross_income = train.groupby('province_code')['gross_income'].apply(lambda x : x.fillna(x.median()))\n# train['gross_income'].isnull().sum()\n# train.gross_income = train.gross_income.fillna(train.gross_income.median())\n# train['gross_income'].isnull().sum()\n# print(\"Null columns\\n\",train.isnull().sum())\n# train.loc[(train['gender'].isna()) & (train['customer_code']%2 == 0),'gender'] = 'H'\n# train.loc[(train['gender'].isna()) & (train['customer_code']%2 == 1),'gender'] = 'V'\n# train.gender.isna().sum()\n# train.drop(columns = ['spouse_index','province_name','last_date_primary'],inplace=True)\n# print(\"Null columns\\n\",train.isnull().sum())\n# train['Date_First_Customer'] = pd.to_datetime(train['Date_First_Customer'])\n# train.channel_by_cust_joined = train.groupby(train.Date_First_Customer.dt.year)['channel_by_cust_joined'].apply(lambda x : x.fillna(x.mode()[0]))\n# train.channel_by_cust_joined.isnull().sum()\n# train.channel_by_cust_joined = train.channel_by_cust_joined.fillna(train.channel_by_cust_joined.mode()[0])\n# train.payroll = train.payroll.fillna(0)\n# train.pensions = train.pensions.fillna(0)\n# train.drop(columns=['customer_type','cust_rel_type'],inplace=True)\n# print(\"Null columns\\n\",train.isnull().sum())# \n# train_sen = train['Seniority']\n# train['Seniority'] = train_sen.astype('int32')\n# train['Date_first_customer_year'] = train['Date_First_Customer'].apply(lambda x : x.year)\n# train\n# train.drop(columns = 'Date_First_Customer',inplace = True)\n# train.drop(columns = 'customer_code',inplace=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}